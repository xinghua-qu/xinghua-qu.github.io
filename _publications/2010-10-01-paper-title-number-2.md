---
title: "Adversary Agnostic Robust Deep Reinforcement Learning"
collection: publications
permalink: /publication/2010-10-01-paper-title-number-1
excerpt: 'We propose an adversary agnostic robust DRL paradigm that does not require learning from predefined adversaries. To this end, we first theoretically show that robustness could indeed be achieved independently of the adversaries based on a policy distillation (PD) setting. Motivated by this finding, we propose a new PD loss with two terms: 1) a prescription gap maximization (PGM) loss aiming to simultaneously maximize the likelihood of the action selected by the teacher policy and the entropy over the remaining actions and 2) a corresponding Jacobian regularization (JR) loss that minimizes the magnitude of gradients with respect to the input state. The theoretical analysis substantiates that our distillation loss guarantees to increase the prescription gap and hence improves the adversarial robustness.'
date: 2021-12-22
venue: 'IEEE Transactions on Neural Networks and Learning Systems'
paperurl: 'https://ieeexplore.ieee.org/document/9660371'
citation: 'Xinghua Qu. (2020). &quot;Adversary Agnostic Robust Deep Reinforcement Learning.&quot; <i>IEEE Transactions on Neural Networks and Learning Systems</i>. 1(1).'
---

